{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc02bf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input a number of paragraphs, bigger than 5, that you want to test ,keep in mind that azure is not free > 5\n",
      "Do you want to use Text Analytics or Translator Detect? write TA or TD > TA\n",
      "Do you want to test your own sentence? write Y or N > Y\n",
      "paste your sentence > Nous déposons des cookies pour optimiser votre expérience utilisateur, améliorer la performance du site, analyser son traffic, vous permettre d'acheter des produits et services relatifs aux évènements sportifs qui vous intéressent, fournir les fonctionnalités des réseaux sociaux et vous montrer notre publicité ciblée, ou celle de nos partenaires.\n",
      "\u001b[32mAccording to AZURE TEXT ANALYTICS, the language of your sentence is  French\n"
     ]
    }
   ],
   "source": [
    "from colorama import Fore, Back, Style\n",
    "\n",
    "#STEP 1: IMPORT DES DONNEES LINGUISTIQUES WIKIPEDIA DE TEST\n",
    "#wikipedia data languages reading\n",
    "#test avec les dataframes de pandas\n",
    "\n",
    "#When reading the txt file with pandas' read_csv, some line loss occurred.\n",
    "#The reason is that there is a single English quotation mark in a line, causing the \\n line break to be invalid.\n",
    "#Multiple lines are concatenated until the next single quote is encountered\n",
    "#add option quoting=3 to tackle the issue :QUOTE_NONE\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dflabel = pd.read_csv('input/labels.csv', delimiter = ';')\n",
    "dfx1 = pd.read_table('input/x_train.txt', delimiter = None, header=None, encoding='utf-8', quoting=3)\n",
    "dfy1 = pd.read_table('input/y_train.txt', delimiter = None, header=None, encoding='utf-8', quoting=3)\n",
    "dfx2 = pd.read_table('input/x_test.txt', delimiter = None, header=None, encoding='utf-8', quoting=3)\n",
    "dfy2 = pd.read_table('input/y_test.txt', delimiter = None, header=None, encoding='utf-8', quoting=3)\n",
    "\n",
    "#row number quick checking\n",
    "#print(len(dfx1.index))\n",
    "#print(len(dfx2.index))\n",
    "#print(len(dfy1.index))\n",
    "#print(len(dfy2.index))\n",
    "#print(len(dflabel.index))\n",
    "\n",
    "#step 2: EXTRACTION DES PARAGRAPHES DES 5 LANGUES LES PLUS PARLEES\n",
    "#5 most spoken languages selection\n",
    "#https://en.wikipedia.org/wiki/List_of_languages_by_total_number_of_speakers\n",
    "#english / mandarin / hindi / spanish / standard arabic\n",
    "\n",
    "#sentences and language merging\n",
    "df1 = pd.concat([dfx1, dfy1], axis=1)\n",
    "df1.columns=[\"para\",\"langcod\"]\n",
    "df2 = pd.concat([dfy2, dfy2], axis=1)\n",
    "df2.columns=[\"para\",\"langcod\"]\n",
    "\n",
    "#5 languages extraction from labels\n",
    "dflabel5mml=dflabel.loc[(dflabel['English']=='Arabic')|\n",
    "                        (dflabel['English']=='English')|\n",
    "                        (dflabel['English']=='Hindi')|\n",
    "                        (dflabel['English']=='Spanish')|\n",
    "                        (dflabel['English']=='Standard Chinese')]\n",
    "dflabel5mml2=dflabel5mml.rename(columns={'Label':'langcod'})\n",
    "dflabel5mml3=dflabel5mml2[['langcod','English']]\n",
    "\n",
    "#sentences selection by most spoken languages-> merging\n",
    "df1s = pd.merge(df1, dflabel5mml3, how=\"inner\", on=[\"langcod\"])\n",
    "df2s = pd.merge(df2, dflabel5mml3, how=\"inner\", on=[\"langcod\"])\n",
    "\n",
    "#row number quick checking\n",
    "#print(len(df1s.index))\n",
    "#print(len(df2s.index))\n",
    "\n",
    "#step 3: EXTRACTION DE N PARAGRAPHES ALEATOIRES POUR NE PAS EXPLOSER LES COMPTEURS AZURE\n",
    "#extraction of n random sentences form dataframe\n",
    "import random\n",
    "\n",
    "def rand_shrink_df(df):\n",
    "    \n",
    "    size = input(\"Input a number of paragraphs, bigger than 5, that you want to test ,\\\n",
    "keep in mind that azure is not free > \")\n",
    "    sizeint = int(size)\n",
    "    #Generate random numbers\n",
    "    #randomlist = random.sample(range(0, len(df.index)-1), sizeint)\n",
    "    randomlist = random.sample(range(0, 500), sizeint)\n",
    "    #print(randomlist)\n",
    "    #trick to get all the language\n",
    "    factor = [(i % 5)*500 for i in range(sizeint)]\n",
    "    #print(factor)\n",
    "    #operation of 2 list with zip\n",
    "    product = [x+y for x,y in zip(randomlist,factor)]\n",
    "    #print(product)\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(sizeint):\n",
    "                            #attention pour que le dataframe reste en row, penser aux double crochets\n",
    "        dfa=df.iloc[[product[i]],:]\n",
    "        #print(i)\n",
    "        if i==0 :\n",
    "            dfaa=dfa\n",
    "            #print(dfaa)\n",
    "            #print(len(dfaa.index))\n",
    "        else:\n",
    "            dfaa=dfaa.append(dfa)\n",
    "            #print(len(dfa.index))\n",
    "    return dfaa\n",
    "\n",
    "df2az=rand_shrink_df(df1s)\n",
    "#print(len(df2az.index))\n",
    "#print(df2az)\n",
    "\n",
    "\n",
    "###################################################################\n",
    "###################################################################\n",
    "\n",
    "\n",
    "#step 4: AUTHENTIFICATION DU CLIENT AU SERVICE cognitive-services\n",
    "#authenticate the client\n",
    "#azure key vault (coffre fort)  to securely store keys\n",
    "\n",
    "#step 5: DETECTION DU LANGAGE VIA L'IA TEXT ANALYTICS D'AZURE ou TRANSLATOR DETECT D'AZURE AU CHOIX\n",
    "#language detection by text analytics\n",
    "import os\n",
    "import cmd\n",
    "import requests, uuid, json\n",
    "\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "def azure_language_detection(df):\n",
    "    \n",
    "    #https://docs.microsoft.com/en-us/python/api/overview/azure/identity-readme?view=azure-python\n",
    "    credential = DefaultAzureCredential()\n",
    "    #get the needed environmental variables to connect to the keyvault\n",
    "    keyVaultName = os.getenv('KEY_VAULT_NAME')\n",
    "    KVUri = f\"https://{keyVaultName}.vault.azure.net\"\n",
    "    client = SecretClient(vault_url=KVUri, credential=credential)\n",
    "\n",
    "    #get the key frome the azure key vault to connect to translator\n",
    "    secretName1 = \"keytranslator\"\n",
    "    key1=client.get_secret(secretName1)\n",
    "     # Add your subscription key and endpoint\n",
    "    subscription_key1 = key1.value\n",
    "    endpoint1 = \"https://api.cognitive.microsofttranslator.com/detect?api-version=3.0\"\n",
    "    location = \"francecentral\"\n",
    "    \n",
    "    headers = {\n",
    "        'Ocp-Apim-Subscription-Key': subscription_key1,\n",
    "        'Content-type': 'application/json',\n",
    "        'Ocp-Apim-Subscription-Region': location,\n",
    "        'X-ClientTraceId': str(uuid.uuid4())\n",
    "    }\n",
    "\n",
    "    \n",
    "    def authenticate_client_ta():\n",
    "        secretName2 = \"keytextanalytics\"\n",
    "        endpoint2 = \"https://ouddane.cognitiveservices.azure.com/\"\n",
    "        key2=client.get_secret(secretName2)\n",
    "        ta_credential = AzureKeyCredential(key2.value)\n",
    "        text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=endpoint2, \n",
    "            credential=ta_credential) \n",
    "        return text_analytics_client\n",
    "    client = authenticate_client_ta()\n",
    "    \n",
    "    srv= input(\"Do you want to use Text Analytics or Translator Detect? write TA or TD > \")\n",
    "    own= input(\"Do you want to test your own sentence? write Y or N > \")\n",
    "    \n",
    "    if srv==\"TA\":\n",
    "        if own==\"Y\":\n",
    "            doc= [input(\"paste your sentence > \")]\n",
    "            response = client.detect_language(documents = doc, country_hint = \"\")[0]\n",
    "            print(Fore.GREEN + \"According to AZURE TEXT ANALYTICS, the language of your sentence is \",response.primary_language.name )\n",
    "        else:\n",
    "            df[\"azure\"] = \"\"\n",
    "            df[\"flag\"]= 0\n",
    "            for i in range(len(df.index)):\n",
    "                try:\n",
    "                    documents = [df.iloc[i,0]] \n",
    "                    response = client.detect_language(documents = documents, country_hint = \"\")[0]\n",
    "                    df.iloc[i,3]=response.primary_language.name\n",
    "                    if (df.iloc[i,3]== \"Chinese_Traditional\") | (df.iloc[i,3]== \"Chinese_Simplified\"):\n",
    "                        df.iloc[i,3]= \"Standard Chinese\"\n",
    "                    if df.iloc[i,3]== df.iloc[i,2]:\n",
    "                        df.iloc[i,4]=1\n",
    "                    else:\n",
    "                        df.iloc[i,4]=0\n",
    "                except Exception as err:\n",
    "                    print(\"Encountered exception. {}\".format(err))\n",
    "                \n",
    "            print(Fore.GREEN + \"TEXT ANALYTICS detection works at\",df[\"flag\"].mean()*100,\"%\")\n",
    "            if df[\"flag\"].mean()<1:\n",
    "                print(\"The error comes maybe from the input data: wrong flag or several languages in the same sentence\")\n",
    "                df0=df.loc[(df['flag']==0)]\n",
    "                print(df0)\n",
    "            print(Style.RESET_ALL)\n",
    "            print(df)\n",
    "            \n",
    "    elif srv==\"TD\":\n",
    "        if own==\"Y\":\n",
    "            doc= input(\"paste your sentence > \")\n",
    "            # You can pass more than one object in json body.\n",
    "            body = [{\n",
    "                'text': doc\n",
    "            }]\n",
    "            ## lib requests\n",
    "            ## post sends a POST request (http) to the specified url and is used when you want to send some data to the server\n",
    "            request = requests.post(endpoint1, headers=headers, json=body)\n",
    "            ##response.json() returns a JSON object of the result / Whenever we make a request to a specified URI through Python, it returns a response object\n",
    "            response = request.json()\n",
    "        \n",
    "            #json.dumps() function converts a Python object into a json string\n",
    "            response2 = json.dumps(response)\n",
    "            #json.loads() method can be used to parse a valid JSON string and convert it into a Python Dictionary\n",
    "            dictresponse = json.loads(response2)[0]\n",
    "\n",
    "            #print(dictresponse)\n",
    "            print(Fore.GREEN + \"According to AZURE TRANSLATOR DETECT, the language of your sentence is \", dictresponse['language'])\n",
    "        \n",
    "        else:\n",
    "            #init flag    \n",
    "            df[\"azure\"] = \"\"\n",
    "            df[\"flag\"]= 0\n",
    "            df[\"azscore\"]=0\n",
    "            for i in range(len(df.index)):\n",
    "               # try:\n",
    "                doc = df.iloc[i,0] \n",
    "                body = [{\n",
    "                       'text': doc\n",
    "                 }]\n",
    "                request = requests.post(endpoint1, headers=headers, json=body)\n",
    "                response = request.json()\n",
    "                response2 = json.dumps(response)\n",
    "                dictresponse = json.loads(response2)[0]\n",
    "                #print(dictresponse)\n",
    "                df.iloc[i,3] = dictresponse['language']\n",
    "                df.iloc[i,5] = dictresponse['score']\n",
    "                if (((df.iloc[i,3]== \"en\") & (df.iloc[i,2]== \"English\"))|\n",
    "                    ((df.iloc[i,3]== \"ar\") & (df.iloc[i,2]== \"Arabic\"))|\n",
    "                    ((df.iloc[i,3]== \"es\") & (df.iloc[i,2]== \"Spanish\"))|\n",
    "                    ((df.iloc[i,3]== \"hi\") & (df.iloc[i,2]== \"Hindi\"))|\n",
    "                    ((df.iloc[i,3]== \"zh-Hant\") & (df.iloc[i,2]== \"Standard Chinese\"))|\n",
    "                    ((df.iloc[i,3]== \"zh-Hans\") & (df.iloc[i,2]== \"Standard Chinese\"))):\n",
    "                    df.iloc[i,4]=1\n",
    "                else:\n",
    "                    df.iloc[i,4]=0\n",
    "               # except Exception as err:\n",
    "               #     print(\"Encountered exception. {}\".format(err))\n",
    "                \n",
    "            print(Fore.GREEN + \"TRANSLATOR detection works at\",df[\"flag\"].mean()*100,\"% according to our indicator and azure average confidence indicator is equal to\",df[\"azscore\"].mean()*100,\"%\")\n",
    "            if df[\"flag\"].mean()<1:\n",
    "                print(\"The error comes maybe from the input data: wrong flag or several languages in the same sentence\")\n",
    "                #65417 de train.txt: plusieurs langues\n",
    "                df0=df.loc[(df['flag']==0)]\n",
    "                print(df0)\n",
    "            print(Style.RESET_ALL)\n",
    "            print(df)\n",
    "azure_language_detection(df2az)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9ce430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5561f674",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
